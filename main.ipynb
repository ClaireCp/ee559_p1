{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "from importlib import reload\n",
    "import generic_helpers\n",
    "reload(generic_helpers)\n",
    "from generic_helpers import *\n",
    "import sys\n",
    "\n",
    "mini_batch_size = 1000\n",
    "nb_runs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1channel2images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"channelimagesModels\")\n",
    "import BaseNet\n",
    "reload(BaseNet)\n",
    "from BaseNet import *\n",
    "import ConvNet1\n",
    "reload(ConvNet1)\n",
    "from ConvNet1 import *\n",
    "import _1channel2images\n",
    "reload(_1channel2images)\n",
    "from _1channel2images import *\n",
    "print(\"Working with 1channel2images framework, nb_classes = \", nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1C_list = [BaseNet1C(), ConvNet1_1C()]\n",
    "lr = 0.01\n",
    "nb_epochs = 100 # way sufficient for this framework\n",
    "for model_1C in model1C_list:\n",
    "    test_results_1C = multiple_training_runs_1C(model_1C, nb_runs, lr, mini_batch_size, nb_epochs, verbose=True)\n",
    "    write_to_csv_1C('1channel2images.csv', model_1C, test_results_1C, lr, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(test_results_1C)\n",
    "write_to_csv_1C('1channel2images.csv', model_1C, test_results_1C, lr, nb_epochs)\n",
    "data = pd.read_csv('1channel2images.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2channels1image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 2channels1image framework, nb_classes =  1\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, \"channelimagesModels\")\n",
    "import BaseNet\n",
    "reload(BaseNet)\n",
    "from BaseNet import *\n",
    "import ConvNet1\n",
    "reload(ConvNet1)\n",
    "from ConvNet1 import *\n",
    "import _2channels1image\n",
    "reload(_2channels1image)\n",
    "from _2channels1image import *\n",
    "import test\n",
    "reload(test)\n",
    "from test import *\n",
    "print(\"Working with 2channels1image framework, nb_classes = \", nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2C_list = [BaseNet2C(), ConvNet1_2C(), ConvNet2_2C()]\n",
    "nb_epochs_list = [100, 200, 150]\n",
    "for (nb_epochs, model_2C) in zip(nb_epochs_list, model2C_list):\n",
    "    test_results_2C = multiple_training_runs_fn(model_2C, train_model_2C, test_model_2C, title_2C, nb_runs, lr, \n",
    "                                                mini_batch_size, nb_epochs, verbose=True)\n",
    "    write_to_csv('2channels1image.csv', model_2C, test_results_2C, lr, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_results_2C)\n",
    "write_to_csv('2channels1image.csv', model_2C, test_results_2C, lr, nb_epochs)\n",
    "data = pd.read_csv('2channels1image.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"weightssharingModels\")\n",
    "import NetSharing\n",
    "reload(NetSharing)\n",
    "from NetSharing import *\n",
    "import weight_sharing\n",
    "reload(weight_sharing)\n",
    "from weight_sharing import *\n",
    "print(\"Working with weight_sharing framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelws_list = [NetSharing1(), NetSharing2(), NetSharing3()] # Stronger reg on Net3\n",
    "nb_epochs_list = [150, 150, 200]\n",
    "for (nb_epochs, model_ws) in zip(nb_epochs_list, modelws_list):\n",
    "    test_results_ws = multiple_training_runs_fn(model_ws, train_model_ws, test_model_ws, title_ws, nb_runs, lr, \n",
    "                                                 mini_batch_size, nb_epochs, verbose=True)\n",
    "    write_to_csv('weightsharing.csv', model_ws, test_results_ws, lr, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(test_results_ws)\n",
    "write_to_csv('weightsharing.csv', model_ws, test_results_ws, lr, nb_epochs)\n",
    "data = pd.read_csv('weightsharing.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with auxiliary losses framework\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, \"auxiliarylossesModels\")\n",
    "import Incept\n",
    "reload(Incept)\n",
    "from Incept import *\n",
    "import auxiliary_losses\n",
    "reload(auxiliary_losses)\n",
    "from auxiliary_losses import *\n",
    "import generic_helpers\n",
    "reload(generic_helpers)\n",
    "from generic_helpers import *\n",
    "print(\"Working with auxiliary losses framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: train, epoch: 100, loss: 3.89312, acc: 0.7660\n",
      "phase: val, epoch: 100, loss: 3.88472, acc: 0.7540\n",
      "Training complete in 1 min 35 s\n",
      "Best val acc: 0.7660\n",
      "phase: train, epoch: 100, loss: 3.76375, acc: 0.7730\n",
      "phase: val, epoch: 100, loss: 3.75537, acc: 0.7750\n",
      "Training complete in 1 min 31 s\n",
      "Best val acc: 0.7750\n",
      "phase: train, epoch: 100, loss: 3.63344, acc: 0.7880\n",
      "phase: val, epoch: 100, loss: 3.62708, acc: 0.7750\n",
      "Training complete in 3 min 40 s\n",
      "Best val acc: 0.7880\n",
      "phase: train, epoch: 100, loss: 3.83732, acc: 0.6820\n",
      "phase: val, epoch: 100, loss: 3.67459, acc: 0.7840\n",
      "Training complete in 3 min 24 s\n",
      "Best val acc: 0.7890\n",
      "phase: train, epoch: 100, loss: 4.40696, acc: 0.6130\n",
      "phase: val, epoch: 100, loss: 4.22997, acc: 0.6520\n",
      "phase: train, epoch: 200, loss: 4.01648, acc: 0.7070\n",
      "phase: val, epoch: 200, loss: 3.80100, acc: 0.7360\n",
      "phase: train, epoch: 300, loss: 3.76933, acc: 0.7480\n",
      "phase: val, epoch: 300, loss: 3.45045, acc: 0.8060\n",
      "phase: train, epoch: 400, loss: 3.49677, acc: 0.7970\n",
      "phase: val, epoch: 400, loss: 3.05521, acc: 0.8720\n",
      "phase: train, epoch: 500, loss: 3.22430, acc: 0.8600\n",
      "phase: val, epoch: 500, loss: 2.63153, acc: 0.9420\n",
      "Training complete in 5 min 30 s\n",
      "Best val acc: 0.9430\n",
      "phase: train, epoch: 100, loss: 4.38719, acc: 0.6140\n",
      "phase: val, epoch: 100, loss: 4.22950, acc: 0.6520\n",
      "phase: train, epoch: 200, loss: 4.02718, acc: 0.7090\n",
      "phase: val, epoch: 200, loss: 3.78708, acc: 0.7400\n",
      "phase: train, epoch: 300, loss: 3.75039, acc: 0.7430\n",
      "phase: val, epoch: 300, loss: 3.48528, acc: 0.7770\n",
      "phase: train, epoch: 400, loss: 3.55648, acc: 0.7840\n",
      "phase: val, epoch: 400, loss: 3.15271, acc: 0.8260\n"
     ]
    }
   ],
   "source": [
    "modelaux_list = [Incept1(), Incept2(), Incept3(), Incept4()] # Stronger reg and longer training on Incept3\n",
    "nb_epochs_list = [100, 100, 500, 100]\n",
    "lr = 0.01\n",
    "\n",
    "#modelaux_list = [Incept3()]\n",
    "#nb_epochs = 300\n",
    "#for model_aux in modelaux_list:\n",
    "    \n",
    "for nb_epochs, model_aux in zip(nb_epochs_list, modelaux_list):\n",
    "    test_results_aux = multiple_training_runs_fn(model_aux, train_model_aux, test_model_aux, title_aux, nb_runs, lr,\n",
    "                                                 mini_batch_size, nb_epochs, verbose=True)\n",
    "    write_to_csv('auxiliary_losses.csv', model_aux, test_results_aux, lr, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(test_results_aux)\n",
    "write_to_csv('auxiliary_losses.csv', model_aux, test_results_aux, lr, nb_epochs)\n",
    "data = pd.read_csv('auxiliary_losses.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
