{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Relevant imports and global variables \"\"\"\n",
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "from generic_helpers import *\n",
    "import sys\n",
    "\n",
    "mini_batch_size = 250\n",
    "nb_runs = 20\n",
    "lr = 0.01\n",
    "\n",
    "######### _1CHANNEL2IMAGES #############################################################\n",
    "sys.path.insert(0, \"channelimagesModels\")\n",
    "from BaseNet import *\n",
    "from ConvNet1 import *\n",
    "from _1channel2images import *\n",
    "print(\"Working with 1channel2images framework, nb_classes = \", nb_classes)\n",
    "\n",
    "model1C_list = [BaseNet1C(), ConvNet1_1C()]\n",
    "nb_epochs = 30\n",
    "for model_1C in model1C_list:\n",
    "    test_results_1C = multiple_training_runs_1C(model_1C, nb_runs, lr, mini_batch_size, nb_epochs, verbose=False)\n",
    "    write_to_csv_1C('1channel2images.csv', model_1C, test_results_1C, lr, nb_epochs)\n",
    "    print(\"For model {}, mean test accuracy = {}\".format(model_1C.name, test_results_1C[6]))\n",
    "    \n",
    "    \n",
    "######### _2CHANNELS1IMAGE #############################################################\n",
    "from _2channels1image import *\n",
    "print(\"Working with 2channels1image framework, nb_classes = \", nb_classes)\n",
    "model2C_list = [BaseNet2C(), ConvNet1_2C(), ConvNet2_2C(), ConvNet4_2C()]\n",
    "nb_epochs = 75\n",
    "for model_2C in model2C_list:\n",
    "    test_results_2C = multiple_training_runs_fn(model_2C, train_model_2C, test_model_2C, title_2C, nb_runs, lr, mini_batch_size, nb_epochs, verbose=False)\n",
    "    write_to_csv('2channels1image.csv', model_2C, test_results_2C, lr, nb_epochs)\n",
    "    print(\"For model {}, mean test accuracy = {}\".format(model_2C.name, test_results_2C[4]))\n",
    "    \n",
    "    \n",
    "######### WEIGHT_SHARING ###############################################################\n",
    "sys.path.insert(0, \"weightssharingModels\")\n",
    "from NetSharing import *\n",
    "from weight_sharing import *\n",
    "print(\"Working with weight_sharing framework\")\n",
    "modelws_list = [NetSharing1(), NetSharing2(), NetSharing3(), NetSharing4()]\n",
    "nb_epochs = 75\n",
    "for model_ws in modelws_list:\n",
    "    test_results_ws = multiple_training_runs_fn(model_ws, train_model_ws, test_model_ws, title_ws, nb_runs, lr, mini_batch_size, nb_epochs, verbose=False)\n",
    "    write_to_csv('weightsharing.csv', model_ws, test_results_ws, lr, nb_epochs)\n",
    "    print(\"For model {}, mean test accuracy = {}\".format(model_ws.name, test_results_ws[4]))\n",
    "\n",
    "    \n",
    "######### AUXILIARY_LOSSES #############################################################\n",
    "sys.path.insert(0, \"auxiliarylossesModels\")  \n",
    "from Incept import *\n",
    "from auxiliary_losses import *\n",
    "print(\"Working with auxiliary_losses framework\")\n",
    "modelaux_list = [Incept1a(), Incept1b(), Incept2(), Incept3(), Incept4()]\n",
    "nb_epochs = 60\n",
    "for model_aux in modelaux_list:\n",
    "    test_results_aux = multiple_training_runs_fn(model_aux, train_model_aux, test_model_aux, title_aux, nb_runs, lr, mini_batch_size, nb_epochs, verbose=False)\n",
    "    write_to_csv('auxiliary_losses.csv', model_aux, test_results_aux, lr, nb_epochs)\n",
    "    print(\"For model {}, mean test accuracy = {}\".format(model_aux.name, test_results_aux[4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
